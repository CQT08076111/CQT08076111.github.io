<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[用halcon实现对饼干的缺陷检测]]></title>
    <url>%2F2019%2F10%2F22%2F%E7%94%A8halcon%E5%AE%9E%E7%8E%B0%E9%A5%BC%E5%B9%B2%E7%9A%84%E7%BC%BA%E9%99%B7%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[使用halcon，用拟合，频域处理等方法对饼干的形状及表面裂痕等缺陷进行检测饼干 检测结果 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133* Image Acquisition 01: Code generated by Image Acquisition 01read_image (Image, 'E:/超人视觉halcon学习/第一套-超人视觉7000元初级和强化班/2 超人Halcon 强化班 理论+实战教程/个人项目/饼干.jpg')dev_close_window()dev_open_window_fit_image (Image, 0, 0, -1, -1, WindowHandle)set_display_font (WindowHandle, 14, 'mono', 'true', 'false')dev_set_line_width (3)dev_display(Image)dev_open_window_fit_image (Image, 0, 0, -1, -1, WindowHandle1)*通道分离decompose3(Image, ImageRed, ImageGreen, ImageBlue)threshold (ImageBlue, Regions, 0, 74)connection(Regions, ConnectedRegions)closing_circle(ConnectedRegions, RegionClosing,20)select_shape (RegionClosing, SelectedRegions, 'area', 'and', 85127.2, 2e+006)*计算区域的空穴面积。area_holes(SelectedRegions, Area)*如果area不为0，有空洞缺陷if(Area &gt; 0) endifcount_obj (SelectedRegions, Number)dev_display (Image)j := 0*根据要提取的线的最大宽度18和对比度19是对比度即最暗的点与最亮的点的像素差计算lines_gauss的参数Sigma、Low和High。calculate_lines_gauss_parameters (18, 20, Sigma, Low, High)for i := 1 to Number by 1 *注意这里是设置显示的窗口，如果不设置之前的显示可能会被清除 dev_set_window (WindowHandle1) dev_set_line_width (1) dev_set_color ('blue') dev_set_draw ('margin') *从对象元组中选择对象。 select_obj (SelectedRegions, ObjectSelected, i) *从区域生成XLD轮廓。 gen_contour_region_xld(ObjectSelected, Contours, 'border') *返回XLD轮廓的坐标。 get_contour_xld (Contours, Row_R, Col_R) *3拟合 fit_circle_contour_xld(Contours, 'algebraic', -1, 0, 0, 3, 2, Row, Column, Radius, StartPhi, EndPhi, PointOrder) *创建一个矩形的XLD轮廓 gen_circle_contour_xld(ContCircle, Row, Column, Radius, 0, EndPhi, 'positive', 1) *返回XLD轮廓的坐标。 get_contour_xld(ContCircle, Row1, Col) *计算所有等高线点到一个椭圆的距离。（计算轮廓上的每一个点到最小外接圆形的最小距离） dist_ellipse_contour_points_xld(Contours, 'unsigned', 0,Row, Column, EndPhi, Radius, Radius, Distances) *判断饼干轮廓上所有点到最小外接矩形的最小距离是否小于30，小于则正常大于则说明该处存在凹陷 RectangleOK := max(Distances) &lt;= 30 *做判断，排除有缺陷的饼干 if (RectangleOK) reduce_domain(ImageRed, ObjectSelected, ImageReduced) area_center(ObjectSelected, Area2, Row3, Column3) *将区域截取出来，这里注意reduce_domain函数生成的ImageReduced只是显示选中的区域其他部分显示为0，但图片面积还是整张图片面积 *这种情况下进行傅里叶变换是将整张图片的像素也进行傅里叶变换，而crop_domain函数是直接将选中的区域截取出来，这样可以只对感兴趣的区域进行傅里叶变换 crop_domain(ImageReduced, ImagePart) area_center(ImagePart, Area3, Row4, Column4) *用频域处理和线段检测，检测饼干上面的裂缝 fft_generic (ImagePart, ImageFFT, 'to_freq', -1,'none', 'dc_center','complex') get_image_size(ImageFFT, Width1, Height1) gen_gauss_filter (ImageGauss,5,5, 0, 'n', 'dc_center', Width1, Height1) convol_fft (ImageFFT, ImageGauss, ImageConvol) * 注意这里的第四个值要为1，如果为-1，则生成的空域跟原图y轴相反 fft_generic (ImageConvol, ImageFFT1, 'from_freq', 1,'none', 'dc_center','byte') sub_image (ImagePart, ImageFFT1, ImageSub, 2, 100) *前面的操作是为了让缺陷更明显 * 将图像放大一个给定的倍数。 ScaleFactor := 0.4 zoom_image_factor (ImageSub, ImageZoomed, ScaleFactor, ScaleFactor, 'constant') *得到图像的域。下面三个函数的作用是使用高斯时避免边界效果 get_domain (ImageZoomed, Domain) erosion_circle(Domain, RegionErosion, 5) reduce_domain (ImageZoomed, RegionErosion, ImageReduced1) *检测线条及其宽度 lines_gauss (ImageReduced1, Lines, Sigma, Low, High, 'dark', 'false', 'bar-shaped', 'false') select_shape_xld (Lines, SelectedXLD, 'contlength', 'and', 34.111, 196.599) *这里要经过仿射变换才能在图中裂缝的位置显示裂缝 hom_mat2d_identity (HomMat2DIdentity) *向齐次2D变换矩阵添加平移 hom_mat2d_translate(HomMat2DIdentity, Row3-Row4,Column3-Column4, HomMat2DTranslate) hom_mat2d_scale_local (HomMat2DTranslate, 1 / ScaleFactor, 1 / ScaleFactor, HomMat2DScale) *对XLD轮廓应用任意仿射2D变换。 affine_trans_contour_xld (SelectedXLD, Defects, HomMat2DScale) *注意这里是设置显示的窗口，如果不设置之前的显示可能会被清除 dev_set_window (WindowHandle) dev_set_line_width (3) dev_set_color ('red') dev_display (Defects) dev_display(Contours) *提取的线有些断开所以要做共线联合 count_obj (Defects, Number) if (Number&gt;0) dev_set_color ('red') set_tposition (WindowHandle, Row, Column) write_string (WindowHandle, 'NOT OK') area_center(ObjectSelected, Area1, Row2, Column2) set_tposition (WindowHandle, Row2-100, Column2-50) write_string (WindowHandle, max(Distances)) set_tposition (WindowHandle, Row2+100, Column2) write_string (WindowHandle, 'have cracks') *disp_message (WindowHandle, max(Distances), 'window', 20+40*i, 20, 'black', 'true') else dev_set_color ('green') set_tposition (WindowHandle, Row, Column) write_string (WindowHandle, 'OK') area_center(ObjectSelected, Area1, Row2, Column2) set_tposition (WindowHandle, Row2-100, Column2-50) write_string (WindowHandle, max(Distances)) *disp_message (WindowHandle, max(Distances), 'window', 20+40*i, 20, 'black', 'true') endif else *注意这里是设置显示的窗口，如果不设置之前的显示可能会被清除 dev_set_window (WindowHandle) dev_set_line_width (3) dev_set_color ('red') dev_display(Contours) set_tposition (WindowHandle, Row, Column) write_string (WindowHandle, 'NOT OK') area_center(ObjectSelected, Area1, Row2, Column2) set_tposition (WindowHandle, Row2-100, Column2-50) write_string (WindowHandle, max(Distances)) *disp_message (WindowHandle, max(Distances), 'window', 20+40*i, 20, 'red', 'true') j := j + 1 endif endfor]]></content>
      <categories>
        <category>智能</category>
      </categories>
      <tags>
        <tag>智能</tag>
        <tag>halcon</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用halcon实现手机主板的定位测量及字符识别]]></title>
    <url>%2F2019%2F10%2F09%2F%E7%94%A8halcon%E5%AE%9E%E7%8E%B0%E6%89%8B%E6%9C%BA%E4%B8%BB%E6%9D%BF%E7%9A%84%E5%AE%9A%E4%BD%8D%E6%B5%8B%E9%87%8F%E5%8F%8A%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[使用halcon对手机主板上的各种部位进行测量和对主板上面的字符进行OCR识别手机主板 实现结果 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418*定位+一维测量+拟合测量+OCR识别(助手与圆环)*定位* Image Acquisition 03: Code generated by Image Acquisition 03read_image (Image, '个人项目/手机主板.jpg')*get_image_size(Image, Width, Height)dev_close_window()dev_open_window(0, 0,800, 500, 'black', WindowHandle)dev_display(Image)dev_set_draw ('margin')dev_set_color('blue')dev_set_line_width (3)set_display_font (WindowHandle, 12, 'mono', 'true', 'false')rgb1_to_gray(Image, GrayImage)*像素值取反invert_image(GrayImage, ImageInvert)threshold (ImageInvert, Regions, 35, 255)connection(Regions, ConnectedRegions)select_shape (ConnectedRegions, SelectedRegions, 'area', 'and', 99611.3, 200000)*（下面仿射变换水平和平移，本人有意用两种操作，纯粹是为了练习，实际使用可以用一种操作）*进行仿射变换让它变水平*得到中心坐标area_center(SelectedRegions, Area, Row, Column)*得到水平方向orientation_region(SelectedRegions, Phi)dev_set_color('red')*在窗口中显示交叉disp_cross (WindowHandle, Row, Column, 15, 0)*在窗口中显示箭头。disp_arrow (WindowHandle, Row, Column, Row - 60 * sin(Phi), Column + 60 * cos(Phi), 2)if (Phi&gt;0) angle:=3.14else angle:=0endif*因为上面检测到的Phi为负数所以这里设置3.14的话是把目标摆水平vector_angle_to_rigid(Row, Column, Phi, Row, Column,angle, HomMat2D)*对图进行仿射变换affine_trans_image(Image, ImageAffinTrans, HomMat2D, 'constant', 'false')*对区域进行仿射变换affine_trans_region(SelectedRegions, RegionAffineTrans, HomMat2D, 'nearest_neighbor')*求矩形凸壳shape_trans(RegionAffineTrans, RegionTrans, 'rectangle1')*仿射变换，使选中的区域平移在窗口的（50，50）位置（测量区域）*最小矩形，目的求区域位置smallest_rectangle1(RegionTrans, Row1, Column1, Row2, Column2)hom_mat2d_identity (HomMat2DIdentity)*向齐次2D变换矩阵添加平移。hom_mat2d_translate (HomMat2DIdentity, 50-Row1, 50-Column1, HomMat2DTranslate)affine_trans_image(ImageAffinTrans, ImageAffinTrans1, HomMat2DTranslate, 'constant', 'false')affine_trans_region(RegionTrans, RegionAffineTrans1, HomMat2DTranslate, 'nearest_neighbor')reduce_domain(ImageAffinTrans1, RegionAffineTrans1, ImageReduced)*开始一维测量* Measure 01: Code generated by Measure 01* Measure 01: Prepare measurementAmplitudeThreshold := 24RoiWidthLen2 := 6set_system ('int_zooming', 'true')* Measure 01: Coordinates for line Measure 01 [0]LineRowStart_Measure_01_0 := 134.5LineColumnStart_Measure_01_0 := 336.5LineRowEnd_Measure_01_0 := 237.5LineColumnEnd_Measure_01_0 := 335.5* Measure 01: Convert coordinates to rectangle2 typeTmpCtrl_Row := 0.5*(LineRowStart_Measure_01_0+LineRowEnd_Measure_01_0)TmpCtrl_Column := 0.5*(LineColumnStart_Measure_01_0+LineColumnEnd_Measure_01_0)TmpCtrl_Dr := LineRowStart_Measure_01_0-LineRowEnd_Measure_01_0TmpCtrl_Dc := LineColumnEnd_Measure_01_0-LineColumnStart_Measure_01_0TmpCtrl_Phi0 := atan2(TmpCtrl_Dr, TmpCtrl_Dc)TmpCtrl_Len1 := 0.5*sqrt(TmpCtrl_Dr*TmpCtrl_Dr + TmpCtrl_Dc*TmpCtrl_Dc)TmpCtrl_Len20 := RoiWidthLen2* Measure 01: Create measure for line Measure 01 [0]* Measure 01: Attention: This assumes all images have the same size!*准备提取垂直于矩形的直边。gen_measure_rectangle2 (TmpCtrl_Row, TmpCtrl_Column, TmpCtrl_Phi0, TmpCtrl_Len1, TmpCtrl_Len20, 800, 500, 'nearest_neighbor', MsrHandle_Measure_01_0)*创建一个矩形。这一步目的只是得到矩形位置gen_rectangle2(Rectangle0, TmpCtrl_Row, TmpCtrl_Column, TmpCtrl_Phi0, TmpCtrl_Len1, TmpCtrl_Len20)* Measure 01: Coordinates for line Measure 01 [1]LineRowStart_Measure_01_1 := 181.5LineColumnStart_Measure_01_1 := 293.5LineRowEnd_Measure_01_1 := 181.5LineColumnEnd_Measure_01_1 := 379.5* Measure 01: Convert coordinates to rectangle2 typeTmpCtrl_Row := 0.5*(LineRowStart_Measure_01_1+LineRowEnd_Measure_01_1)TmpCtrl_Column := 0.5*(LineColumnStart_Measure_01_1+LineColumnEnd_Measure_01_1)TmpCtrl_Dr := LineRowStart_Measure_01_1-LineRowEnd_Measure_01_1TmpCtrl_Dc := LineColumnEnd_Measure_01_1-LineColumnStart_Measure_01_1TmpCtrl_Phi1 := atan2(TmpCtrl_Dr, TmpCtrl_Dc)TmpCtrl_Len1 := 0.5*sqrt(TmpCtrl_Dr*TmpCtrl_Dr + TmpCtrl_Dc*TmpCtrl_Dc)TmpCtrl_Len21 := RoiWidthLen2* Measure 01: Create measure for line Measure 01 [1]* Measure 01: Attention: This assumes all images have the same size!gen_measure_rectangle2 (TmpCtrl_Row, TmpCtrl_Column, TmpCtrl_Phi1, TmpCtrl_Len1, TmpCtrl_Len21, 800, 500, 'nearest_neighbor', MsrHandle_Measure_01_1)gen_rectangle2(Rectangle1, TmpCtrl_Row, TmpCtrl_Column, TmpCtrl_Phi1, TmpCtrl_Len1, TmpCtrl_Len21)* Measure 01: Coordinates for line Measure 01 [2]LineRowStart_Measure_01_2 := 89.5LineColumnStart_Measure_01_2 := 427.5LineRowEnd_Measure_01_2 := 87.5LineColumnEnd_Measure_01_2 := 500.5* Measure 01: Convert coordinates to rectangle2 typeTmpCtrl_Row := 0.5*(LineRowStart_Measure_01_2+LineRowEnd_Measure_01_2)TmpCtrl_Column := 0.5*(LineColumnStart_Measure_01_2+LineColumnEnd_Measure_01_2)TmpCtrl_Dr := LineRowStart_Measure_01_2-LineRowEnd_Measure_01_2TmpCtrl_Dc := LineColumnEnd_Measure_01_2-LineColumnStart_Measure_01_2TmpCtrl_Phi2 := atan2(TmpCtrl_Dr, TmpCtrl_Dc)TmpCtrl_Len1 := 0.5*sqrt(TmpCtrl_Dr*TmpCtrl_Dr + TmpCtrl_Dc*TmpCtrl_Dc)TmpCtrl_Len22 := RoiWidthLen2* Measure 01: Create measure for line Measure 01 [2]* Measure 01: Attention: This assumes all images have the same size!gen_measure_rectangle2 (TmpCtrl_Row, TmpCtrl_Column, TmpCtrl_Phi2, TmpCtrl_Len1, TmpCtrl_Len22, 800, 500, 'nearest_neighbor', MsrHandle_Measure_01_2)gen_rectangle2(Rectangle2, TmpCtrl_Row, TmpCtrl_Column, TmpCtrl_Phi2, TmpCtrl_Len1, TmpCtrl_Len22)* Measure 01: Coordinates for line Measure 01 [3]LineRowStart_Measure_01_3 := 72.5LineColumnStart_Measure_01_3 := 86.5LineRowEnd_Measure_01_3 := 72.5LineColumnEnd_Measure_01_3 := 141.5* Measure 01: Convert coordinates to rectangle2 typeTmpCtrl_Row := 0.5*(LineRowStart_Measure_01_3+LineRowEnd_Measure_01_3)TmpCtrl_Column := 0.5*(LineColumnStart_Measure_01_3+LineColumnEnd_Measure_01_3)TmpCtrl_Dr := LineRowStart_Measure_01_3-LineRowEnd_Measure_01_3TmpCtrl_Dc := LineColumnEnd_Measure_01_3-LineColumnStart_Measure_01_3TmpCtrl_Phi3 := atan2(TmpCtrl_Dr, TmpCtrl_Dc)TmpCtrl_Len1 := 0.5*sqrt(TmpCtrl_Dr*TmpCtrl_Dr + TmpCtrl_Dc*TmpCtrl_Dc)TmpCtrl_Len23 := RoiWidthLen2* Measure 01: Create measure for line Measure 01 [3]* Measure 01: Attention: This assumes all images have the same size!gen_measure_rectangle2 (TmpCtrl_Row, TmpCtrl_Column, TmpCtrl_Phi3, TmpCtrl_Len1, TmpCtrl_Len23, 800, 500, 'nearest_neighbor', MsrHandle_Measure_01_3)gen_rectangle2(Rectangle3, TmpCtrl_Row, TmpCtrl_Column, TmpCtrl_Phi3, TmpCtrl_Len1, TmpCtrl_Len23)* Measure 01: Coordinates for line Measure 01 [4]LineRowStart_Measure_01_4 := 227.5LineColumnStart_Measure_01_4 := 139.5LineRowEnd_Measure_01_4 := 315.5LineColumnEnd_Measure_01_4 := 141.5* Measure 01: Convert coordinates to rectangle2 typeTmpCtrl_Row := 0.5*(LineRowStart_Measure_01_4+LineRowEnd_Measure_01_4)TmpCtrl_Column := 0.5*(LineColumnStart_Measure_01_4+LineColumnEnd_Measure_01_4)TmpCtrl_Dr := LineRowStart_Measure_01_4-LineRowEnd_Measure_01_4TmpCtrl_Dc := LineColumnEnd_Measure_01_4-LineColumnStart_Measure_01_4TmpCtrl_Phi4 := atan2(TmpCtrl_Dr, TmpCtrl_Dc)TmpCtrl_Len1 := 0.5*sqrt(TmpCtrl_Dr*TmpCtrl_Dr + TmpCtrl_Dc*TmpCtrl_Dc)TmpCtrl_Len24 := RoiWidthLen2* Measure 01: Create measure for line Measure 01 [4]* Measure 01: Attention: This assumes all images have the same size!gen_measure_rectangle2 (TmpCtrl_Row, TmpCtrl_Column, TmpCtrl_Phi4, TmpCtrl_Len1, TmpCtrl_Len24, 800, 500, 'nearest_neighbor', MsrHandle_Measure_01_4)gen_rectangle2(Rectangle4, TmpCtrl_Row, TmpCtrl_Column, TmpCtrl_Phi4, TmpCtrl_Len1, TmpCtrl_Len24)* Measure 01: Coordinates for line Measure 01 [5]LineRowStart_Measure_01_5 := 75.5LineColumnStart_Measure_01_5 := 539.5LineRowEnd_Measure_01_5 := 75.5LineColumnEnd_Measure_01_5 := 593.5* Measure 01: Convert coordinates to rectangle2 typeTmpCtrl_Row := 0.5*(LineRowStart_Measure_01_5+LineRowEnd_Measure_01_5)TmpCtrl_Column := 0.5*(LineColumnStart_Measure_01_5+LineColumnEnd_Measure_01_5)TmpCtrl_Dr := LineRowStart_Measure_01_5-LineRowEnd_Measure_01_5TmpCtrl_Dc := LineColumnEnd_Measure_01_5-LineColumnStart_Measure_01_5TmpCtrl_Phi5 := atan2(TmpCtrl_Dr, TmpCtrl_Dc)TmpCtrl_Len1 := 0.5*sqrt(TmpCtrl_Dr*TmpCtrl_Dr + TmpCtrl_Dc*TmpCtrl_Dc)TmpCtrl_Len25 := RoiWidthLen2* Measure 01: Create measure for line Measure 01 [5]* Measure 01: Attention: This assumes all images have the same size!gen_measure_rectangle2 (TmpCtrl_Row, TmpCtrl_Column, TmpCtrl_Phi5, TmpCtrl_Len1, TmpCtrl_Len25, 800, 500, 'nearest_neighbor', MsrHandle_Measure_01_5)gen_rectangle2(Rectangle5, TmpCtrl_Row, TmpCtrl_Column, TmpCtrl_Phi5, TmpCtrl_Len1, TmpCtrl_Len25)* Measure 01: Coordinates for line Measure 01 [6]LineRowStart_Measure_01_6 := 253.5LineColumnStart_Measure_01_6 := 445.5LineRowEnd_Measure_01_6 := 322.5LineColumnEnd_Measure_01_6 := 445.5* Measure 01: Convert coordinates to rectangle2 typeTmpCtrl_Row := 0.5*(LineRowStart_Measure_01_6+LineRowEnd_Measure_01_6)TmpCtrl_Column := 0.5*(LineColumnStart_Measure_01_6+LineColumnEnd_Measure_01_6)TmpCtrl_Dr := LineRowStart_Measure_01_6-LineRowEnd_Measure_01_6TmpCtrl_Dc := LineColumnEnd_Measure_01_6-LineColumnStart_Measure_01_6TmpCtrl_Phi6 := atan2(TmpCtrl_Dr, TmpCtrl_Dc)TmpCtrl_Len1 := 0.5*sqrt(TmpCtrl_Dr*TmpCtrl_Dr + TmpCtrl_Dc*TmpCtrl_Dc)TmpCtrl_Len26 := RoiWidthLen2* Measure 01: Create measure for line Measure 01 [6]* Measure 01: Attention: This assumes all images have the same size!gen_measure_rectangle2 (TmpCtrl_Row, TmpCtrl_Column, TmpCtrl_Phi6, TmpCtrl_Len1, TmpCtrl_Len26, 800, 500, 'nearest_neighbor', MsrHandle_Measure_01_6)gen_rectangle2(Rectangle6, TmpCtrl_Row, TmpCtrl_Column, TmpCtrl_Phi6, TmpCtrl_Len1, TmpCtrl_Len26)* Measure 01: **************************************************************** Measure 01: * The code which follows is to be executed once / measurement ** Measure 01: **************************************************************** Measure 01: The image is assumed to be made available in the* Measure 01: variable last displayed in the graphics window*复制一个图象。copy_obj (ImageReduced, Image, 1, 1)* Measure 01: Execute measurements*提取垂直于矩形或环形弧的直边对。measure_pairs (Image, MsrHandle_Measure_01_0, 0.4, 24, 'all', 'all', Row1_Measure_01_0, Column1_Measure_01_0, Amplitude1_Measure_01_0, Row2_Measure_01_0, Column2_Measure_01_0, Amplitude2_Measure_01_0, Width_Measure_01_0, Distance_Measure_01_0)measure_pairs (Image, MsrHandle_Measure_01_1, 0.4, 24, 'all', 'all', Row1_Measure_01_1, Column1_Measure_01_1, Amplitude1_Measure_01_1, Row2_Measure_01_1, Column2_Measure_01_1, Amplitude2_Measure_01_1, Width_Measure_01_1, Distance_Measure_01_1)measure_pairs (Image, MsrHandle_Measure_01_2, 0.4, 24, 'all', 'all', Row1_Measure_01_2, Column1_Measure_01_2, Amplitude1_Measure_01_2, Row2_Measure_01_2, Column2_Measure_01_2, Amplitude2_Measure_01_2, Width_Measure_01_2, Distance_Measure_01_2)measure_pairs (Image, MsrHandle_Measure_01_3, 0.4, 24, 'all', 'all', Row1_Measure_01_3, Column1_Measure_01_3, Amplitude1_Measure_01_3, Row2_Measure_01_3, Column2_Measure_01_3, Amplitude2_Measure_01_3, Width_Measure_01_3, Distance_Measure_01_3)measure_pairs (Image, MsrHandle_Measure_01_4, 0.4, 24, 'all', 'all', Row1_Measure_01_4, Column1_Measure_01_4, Amplitude1_Measure_01_4, Row2_Measure_01_4, Column2_Measure_01_4, Amplitude2_Measure_01_4, Width_Measure_01_4, Distance_Measure_01_4)measure_pairs (Image, MsrHandle_Measure_01_5, 0.4, 24, 'all', 'all', Row1_Measure_01_5, Column1_Measure_01_5, Amplitude1_Measure_01_5, Row2_Measure_01_5, Column2_Measure_01_5, Amplitude2_Measure_01_5, Width_Measure_01_5, Distance_Measure_01_5)measure_pairs (Image, MsrHandle_Measure_01_6, 0.4, 24, 'all', 'all', Row1_Measure_01_6, Column1_Measure_01_6, Amplitude1_Measure_01_6, Row2_Measure_01_6, Column2_Measure_01_6, Amplitude2_Measure_01_6, Width_Measure_01_6, Distance_Measure_01_6)dev_display(Rectangle0)dev_display(Rectangle1)dev_display(Rectangle2)dev_display(Rectangle3)dev_display(Rectangle4)dev_display(Rectangle5)dev_display(Rectangle6)*显示测量的两边（下面函数是自建函数可以按F7键显示源码）*显示正测量边p_disp_edge_marker (Row1_Measure_01_0, Column1_Measure_01_0, TmpCtrl_Phi0, TmpCtrl_Len20, 'blue', 2)p_disp_edge_marker (Row1_Measure_01_1, Column1_Measure_01_1, TmpCtrl_Phi1, TmpCtrl_Len21, 'blue', 2)p_disp_edge_marker (Row1_Measure_01_2, Column1_Measure_01_2, TmpCtrl_Phi2, TmpCtrl_Len22, 'blue', 2)p_disp_edge_marker (Row1_Measure_01_3, Column1_Measure_01_3, TmpCtrl_Phi3, TmpCtrl_Len23, 'blue', 2)p_disp_edge_marker (Row1_Measure_01_4, Column1_Measure_01_4, TmpCtrl_Phi4, TmpCtrl_Len24, 'blue', 2)p_disp_edge_marker (Row1_Measure_01_5, Column1_Measure_01_5, TmpCtrl_Phi5, TmpCtrl_Len25, 'blue', 2)p_disp_edge_marker (Row1_Measure_01_6, Column1_Measure_01_6, TmpCtrl_Phi6, TmpCtrl_Len26, 'blue', 2)*显示负测量边p_disp_edge_marker (Row2_Measure_01_0, Column2_Measure_01_0, TmpCtrl_Phi0, TmpCtrl_Len20, 'blue', 2)p_disp_edge_marker (Row2_Measure_01_1, Column2_Measure_01_1, TmpCtrl_Phi1, TmpCtrl_Len21, 'blue', 2)p_disp_edge_marker (Row2_Measure_01_2, Column2_Measure_01_2, TmpCtrl_Phi2, TmpCtrl_Len22, 'blue', 2)p_disp_edge_marker (Row2_Measure_01_3, Column2_Measure_01_3, TmpCtrl_Phi3, TmpCtrl_Len23, 'blue', 2)p_disp_edge_marker (Row2_Measure_01_4, Column2_Measure_01_4, TmpCtrl_Phi4, TmpCtrl_Len24, 'blue', 2)p_disp_edge_marker (Row2_Measure_01_5, Column2_Measure_01_5, TmpCtrl_Phi5, TmpCtrl_Len25, 'blue', 2)p_disp_edge_marker (Row2_Measure_01_6, Column2_Measure_01_6, TmpCtrl_Phi6, TmpCtrl_Len26, 'blue', 2)* Measure 01: Do something with the results* Measure 01: Clear measure when doneclose_measure (MsrHandle_Measure_01_0)close_measure (MsrHandle_Measure_01_1)close_measure (MsrHandle_Measure_01_2)close_measure (MsrHandle_Measure_01_3)close_measure (MsrHandle_Measure_01_4)close_measure (MsrHandle_Measure_01_5)close_measure (MsrHandle_Measure_01_6)*拟合测量上面两个角的圆弧rgb1_to_gray(ImageReduced,Imagegray)dev_set_draw ('fill')threshold (Imagegray, Regions1, 0, 220)connection(Regions1, ConnectedRegions1)select_shape (ConnectedRegions1, SelectedRegions1, 'area', 'and', 99805.6, 200000)*获取边缘这里不能直接用edges_sub_pix这个函数提取边缘因为中间边缘太多了*填充孔洞 fill_up (SelectedRegions1, RegionFillUp)*下面两个腐蚀结构元加起来相当于一个“十”字结构元erosion_rectangle1(RegionFillUp, RegionErosion, 3, 1)erosion_rectangle1(RegionErosion, RegionErosion1, 1, 3)*求两个区域的差得到边缘轮廓difference (RegionFillUp, RegionErosion1, RegionDifference) reduce_domain(Imagegray, RegionDifference, ImageReduced1)*使用Deriche, Lanser, Shen或Canny过滤器提取亚像素精确的边缘。edges_sub_pix(ImageReduced1, Edges, 'canny', 1, 20, 30)select_shape_xld (Edges, SelectedXLD, ['height','width'], 'and', [22.012,38.39], [100,200])*分割轮廓；将XLD等高线分割成线段和圆弧或椭圆圆弧。segment_contours_xld(SelectedXLD, ContoursSplit, 'lines_circles', 5, 4, 2)*根据圆度提取出来select_shape_xld (ContoursSplit, SelectedXLD1, 'circularity', 'and', 0.02114, 0.5)*根据轮廓的相对位置排序。sort_contours_xld(SelectedXLD1, SortedContour, 'character', 'true', 'row')*用圆近似XLD的轮廓。（拟合圆）fit_circle_contour_xld(SortedContour, 'algebraic', -1, 0, 0, 3, 2, Row3, Column3, Radius, StartPhi, EndPhi, PointOrder)*创建对应于圆形或圆弧的XLD轮廓（画出拟合圆）。（注意图中的圆更圆了）gen_circle_contour_xld(ContCircle1, Row3[0], Column3[0], Radius[0], 0, 6.28318, 'positive', 1)gen_circle_contour_xld(ContCircle2, Row3[1], Column3[1], Radius[1], 0, 6.28318, 'positive', 1)disp_message (WindowHandle, '左上角圆半径' +Radius[0]+'右上角半径'+Radius[1], 'window',450, 10, 'blue', 'false')*OCR识别助手实现* OCR 01: Code generated by OCR 01* OCR 01: * OCR 01: Prepare text model* OCR 01: create_text_model_reader ('manual', [], TextModel)set_text_model_param (TextModel, 'polarity', 'light_on_dark')set_text_model_param (TextModel, 'char_width', 7)set_text_model_param (TextModel, 'char_height', 24)set_text_model_param (TextModel, 'stroke_width', 2.4)set_text_model_param (TextModel, 'return_punctuation', 'false')set_text_model_param (TextModel, 'return_separators', 'false')set_text_model_param (TextModel, 'uppercase_only', 'true')set_text_model_param (TextModel, 'fragment_size_min', 4)set_text_model_param (TextModel, 'eliminate_border_blobs', 'true')set_text_model_param (TextModel, 'add_fragments', 'false')set_text_model_param (TextModel, 'base_line_tolerance', 0.2)set_text_model_param (TextModel, 'max_line_num', 1)* OCR 01: * OCR 01: Load classifier* OCR 01: read_ocr_class_mlp ('Industrial_0-9A-Z_NoRej.omc', OcrHandle)* OCR 01: **************************************************************** OCR 01: * The code which follows is to be executed once / image ** OCR 01: **************************************************************** OCR 01: * OCR 01: The image is assumed to be made available in the* OCR 01: variable last displayed in the graphics windowcopy_obj (ImageReduced, Image, 1, 1)* OCR 01: * OCR 01: Perform actual processing (once per ROI)* OCR 01: Generate regions of interestgen_rectangle1 (ROI_OCR_01_0, 92.5, 271.5, 118.5, 328.5)* OCR 01: * OCR 01: Extract symbol regions (segmentation step)* OCR 01: Only consider first channel for color imagesaccess_channel (Image, TmpObj_Mono, 1)reduce_domain (TmpObj_Mono, ROI_OCR_01_0, TmpObj_MonoReduced_OCR_01_0)* OCR 01: * OCR 01: Orientation Correction (for consistent border and domain* OCR 01: handling, this is always applied, even with no rotation)* OCR 01: hom_mat2d_identity (TmpCtrl_MatrixIdentity)* OCR 01: Apply transformation to image and domainget_domain (TmpObj_MonoReduced_OCR_01_0, TmpObj_Domain)get_system ('clip_region', TmpCtrl_ClipRegion)set_system ('clip_region', 'false')dilation_circle (TmpObj_Domain, TmpObj_DomainExpanded, 12)*对区域应用任意的仿射2D变换。affine_trans_region (TmpObj_DomainExpanded, TmpObj_DomainTransformedRaw, TmpCtrl_MatrixIdentity, 'true')smallest_rectangle1 (TmpObj_DomainTransformedRaw, TmpCtrl_Row1, TmpCtrl_Col1, TmpCtrl_Row2, TmpCtrl_Col2)hom_mat2d_translate (TmpCtrl_MatrixIdentity, -TmpCtrl_Row1, -TmpCtrl_Col1, TmpCtrl_MatrixTranslation)hom_mat2d_compose (TmpCtrl_MatrixTranslation, TmpCtrl_MatrixIdentity, TmpCtrl_MatrixComposite)affine_trans_region (TmpObj_Domain, TmpObj_DomainTransformed, TmpCtrl_MatrixComposite, 'true')affine_trans_image (TmpObj_MonoReduced_OCR_01_0, TmpObj_ImageTransformed, TmpCtrl_MatrixComposite, 'constant', 'true')dilation_circle (TmpObj_Domain, TmpObj_DomainExpanded, 12)expand_domain_gray (TmpObj_ImageTransformed, TmpObj_ImageTransformedExpanded, 12)reduce_domain (TmpObj_ImageTransformed, TmpObj_DomainTransformed, TmpObj_ImageTransformedReduced)crop_part (TmpObj_ImageTransformedReduced, TmpObj_MonoReduced_OCR_01_0, 0, 0, TmpCtrl_Col2-TmpCtrl_Col1+1, TmpCtrl_Row2-TmpCtrl_Row1+1)set_system ('clip_region', TmpCtrl_ClipRegion)find_text (TmpObj_MonoReduced_OCR_01_0, TextModel, TmpCtrl_ResultHandle_OCR_01_0)* OCR 01: * OCR 01: Read text (classification step)* OCR 01: Gray values for reading must be dark on brightinvert_image (TmpObj_MonoReduced_OCR_01_0, TmpObj_MonoInverted_OCR_01_0)get_text_object (Symbols_OCR_01_0, TmpCtrl_ResultHandle_OCR_01_0, 'all_lines')clear_text_result (TmpCtrl_ResultHandle_OCR_01_0)dev_display (TmpObj_MonoInverted_OCR_01_0)dev_set_draw ('fill')dev_set_colored (3)dev_display (Symbols_OCR_01_0)do_ocr_multi_class_mlp (Symbols_OCR_01_0, TmpObj_MonoInverted_OCR_01_0, OcrHandle, SymbolNames_OCR_01_0, Confidences_OCR_01_0)* OCR 01: * OCR 01: Do something with the results* OCR 01: * OCR 01: Cleanup (global)clear_text_model (TextModel)clear_ocr_class_mlp (OcrHandle)*OCR代码实现圆环字符识别dev_display(ImageReduced)gen_circle (ROI_0, 261.5, 84.5, 21.2603)reduce_domain(ImageReduced, ROI_0, ImageReduced2)rgb1_to_gray(ImageReduced2, GrayImage1)threshold (GrayImage1, Regions2, 140, 228)connection(Regions2, ConnectedRegions2)select_shape (ConnectedRegions2, SelectedRegions2, 'area', 'and', 24.78, 655.49)shape_trans(SelectedRegions2, RegionTrans1, 'convex')*外圆的最小圆形,目的得到半径smallest_circle(RegionTrans1, Row4, Column4, Radius1)*估计小圆的半径为大圆的1/3Radius2:=Radius1/3*3预处理（极坐标转换）将图像中的环形弧转换为极坐标。（这里作用是将圆环拉直）*rad(360), 0分别是裁切的起点的角度和终点的角度值*Radius1+2是字符的外侧到圆心的距离Radius1是外圆环到圆心的距离注意区别；Radius2-3是字符的内侧到圆心的距离Radius2是内圆环到圆心的距离*570是拉直图像后缩放的长， 30是拉直图像后缩放的宽；缩放采用插值算法polar_trans_image_ext (ImageReduced, PolarTransImage, Row4, Column4, rad(360), 0, Radius1-3, Radius2+2, 570, 30, 'bilinear')rgb1_to_gray(PolarTransImage, GrayImage2)threshold (GrayImage2, Regions3, 122, 213)connection(Regions3, ConnectedRegions3)*开操作opening_rectangle1(ConnectedRegions3, RegionOpening, 1,30)*求两个区域的差区域difference(ConnectedRegions3, RegionOpening, RegionDifference1)select_shape (RegionDifference1, SelectedRegions3, 'area', 'and', 191.93, 967.44)*排序sort_region(SelectedRegions3,SortedRegions2, 'character', 'true', 'column')*读入OCR训练好的分类器read_ocr_class_mlp ('C:/Program Files/MVTec/HALCON-12.0/ocr/Industrial_0-9_NoRej.omc', OCRHandle)*预处理（对比度变换）灰度翻转（255-原灰度）,识别前一定要先把图像生成暗字亮底的图像invert_image(PolarTransImage, ImageInvert1)*6识别do_ocr_multi_class_mlp (SortedRegions2, ImageInvert1, OCRHandle, Class, Confidence)clear_ocr_class_mlp (OCRHandle)*显示dev_close_window()dev_open_window(0, 0,800, 500, 'black', WindowHandle)dev_set_draw ('margin')dev_set_line_width (3)dev_display(ImageReduced)dev_display(Rectangle0)dev_display(Rectangle1)dev_display(Rectangle2)dev_display(Rectangle3)dev_display(Rectangle4)dev_display(Rectangle5)dev_display(Rectangle6)*显示正测量边p_disp_edge_marker (Row1_Measure_01_0, Column1_Measure_01_0, TmpCtrl_Phi0, TmpCtrl_Len20, 'blue', 2)p_disp_edge_marker (Row1_Measure_01_1, Column1_Measure_01_1, TmpCtrl_Phi1, TmpCtrl_Len21, 'blue', 2)p_disp_edge_marker (Row1_Measure_01_2, Column1_Measure_01_2, TmpCtrl_Phi2, TmpCtrl_Len22, 'blue', 2)p_disp_edge_marker (Row1_Measure_01_3, Column1_Measure_01_3, TmpCtrl_Phi3, TmpCtrl_Len23, 'blue', 2)p_disp_edge_marker (Row1_Measure_01_4, Column1_Measure_01_4, TmpCtrl_Phi4, TmpCtrl_Len24, 'blue', 2)p_disp_edge_marker (Row1_Measure_01_5, Column1_Measure_01_5, TmpCtrl_Phi5, TmpCtrl_Len25, 'blue', 2)p_disp_edge_marker (Row1_Measure_01_6, Column1_Measure_01_6, TmpCtrl_Phi6, TmpCtrl_Len26, 'blue', 2)*显示负测量边p_disp_edge_marker (Row2_Measure_01_0, Column2_Measure_01_0, TmpCtrl_Phi0, TmpCtrl_Len20, 'blue', 2)p_disp_edge_marker (Row2_Measure_01_1, Column2_Measure_01_1, TmpCtrl_Phi1, TmpCtrl_Len21, 'blue', 2)p_disp_edge_marker (Row2_Measure_01_2, Column2_Measure_01_2, TmpCtrl_Phi2, TmpCtrl_Len22, 'blue', 2)p_disp_edge_marker (Row2_Measure_01_3, Column2_Measure_01_3, TmpCtrl_Phi3, TmpCtrl_Len23, 'blue', 2)p_disp_edge_marker (Row2_Measure_01_4, Column2_Measure_01_4, TmpCtrl_Phi4, TmpCtrl_Len24, 'blue', 2)p_disp_edge_marker (Row2_Measure_01_5, Column2_Measure_01_5, TmpCtrl_Phi5, TmpCtrl_Len25, 'blue', 2)p_disp_edge_marker (Row2_Measure_01_6, Column2_Measure_01_6, TmpCtrl_Phi6, TmpCtrl_Len26, 'blue', 2)dev_display(ContCircle1)*在窗口中显示圆的中心的点disp_cross (WindowHandle,Row3[0], Column3[0], 10, 0)dev_display(ContCircle2)disp_cross (WindowHandle,Row3[1], Column3[1], 10, 0)dev_display(RegionTrans1)dev_display(ROI_OCR_01_0)disp_message (WindowHandle,SymbolNames_OCR_01_0,'SymbolNames_OCR_01_0', 80, 240, 'black', 'true')Row6:=Row4+30Column6:=Column4-15disp_message (WindowHandle, Class, 'PolarTransImage', Row6, Column6, 'black', 'true')]]></content>
      <categories>
        <category>智能</category>
      </categories>
      <tags>
        <tag>智能</tag>
        <tag>halcon</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取淘宝各品牌手机月销量数据]]></title>
    <url>%2F2019%2F05%2F26%2F%E7%88%AC%E5%8F%96%E6%B7%98%E5%AE%9D%E5%90%84%E5%93%81%E7%89%8C%E6%89%8B%E6%9C%BA%E6%9C%88%E9%94%80%E9%87%8F%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[原理使用scrapy框架搭建爬虫对接selenium模拟chrome浏览器对taobao网站各品牌的手机销量，价格等数据进行爬取。并将数据分别保存在MySql与MongoDB数据库中，再使用pyecharts库对数据进行可视化。 搭建淘宝爬虫爬取数据1234567891011121314151617181920212223242526from scrapy import Request,Spiderfrom urllib.parse import quotefrom scrapytaobao.items import ProductItemclass TaobaoSpider(Spider): name = 'taobao' allowed_domains = ['www.taobao.com'] base_url = 'http://s.taobao.com/search?q=' def start_requests(self): for keyword in self.settings.get('KEYWORDS'): for page in range(1,self.settings.get('MAX_PAGE')+1): url=self.base_url+quote(keyword)+'&amp;sort=sale-desc' yield Request(url=url,callback=self.parse,meta=&#123;'page':page&#125;,dont_filter=True) def parse(self, response): products = response.xpath( '//div[@id="mainsrp-itemlist"]//div[@class="items"][1]//div[contains(@class, "item")]') for product in products: item = ProductItem() item['title'] = ''.join(product.xpath('.//div[contains(@class, "title")]//text()').extract()).strip() item['price'] = ''.join(product.xpath('.//div[contains(@class, "price")]//text()').extract()).strip() item['shop'] = ''.join(product.xpath('.//div[contains(@class, "shop")]//text()').extract()).strip() item['image'] = ''.join(product.xpath('.//div[@class="pic"]//img[contains(@class, "img")]/@data-src').extract()).strip() item['deal'] = product.xpath('.//div[contains(@class, "deal-cnt")]//text()').extract_first() item['location'] = product.xpath('.//div[contains(@class, "location")]//text()').extract_first() yield item 使用selenium模拟浏览器操作1234567891011121314151617181920212223242526272829303132333435363738394041424344from selenium import webdriverfrom selenium.common.exceptions import TimeoutExceptionfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom scrapy.http import HtmlResponsefrom logging import getLoggerclass SeleniumMiddleware(): def __init__(self, timeout=None, service_args=[]): self.logger = getLogger(__name__) self.timeout = timeout self.browser = webdriver.Chrome() self.wait = WebDriverWait(self.browser, self.timeout) def __del__(self): self.browser.close() def process_request(self, request, spider): self.logger.debug('PhantomJS is Starting') page = request.meta.get('page', 1) try: self.browser.get(request.url) if page &gt; 1: input = self.wait.until( EC.presence_of_element_located((By.CSS_SELECTOR, '#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; input'))) submit = self.wait.until( EC.element_to_be_clickable((By.CSS_SELECTOR, '#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; span.btn.J_Submit'))) input.clear() input.send_keys(page) submit.click() self.wait.until( EC.text_to_be_present_in_element((By.CSS_SELECTOR, '#mainsrp-pager &gt; div &gt; div &gt; div &gt; ul &gt; li.item.active &gt; span'), str(page))) #self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.m-itemlist .items .item'))) return HtmlResponse(url=request.url, body=self.browser.page_source, request=request, encoding='utf-8', status=200) except TimeoutException: return HtmlResponse(url=request.url, status=500, request=request) @classmethod def from_crawler(cls, crawler): return cls(timeout=crawler.settings.get('SELENIUM_TIMEOUT'), service_args=crawler.settings.get('PHANTOMJS_SERVICE_ARGS')) 将数据保存到MySql与MongoDB数据库中1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import pymongoimport pymysqlclass MongoPipeline(object): def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls(mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DB')) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def process_item(self, item, spider): self.db[item.collection].insert(dict(item)) return item def close_spider(self,spider): self.client.close()class MysqlPipeline(): def __init__(self, host, database, user, password, port,table): self.host = host self.database = database self.user = user self.password = password self.port = port self.table= table @classmethod def from_crawler(cls, crawler): return cls( host=crawler.settings.get('MYSQL_HOST'), database=crawler.settings.get('MYSQL_DATABASE'), user=crawler.settings.get('MYSQL_USER'), password=crawler.settings.get('MYSQL_PASSWORD'), port=crawler.settings.get('MYSQL_PORT'), table=crawler.settings.get('TABLE') ) def open_spider(self, spider): self.db = pymysql.connect(self.host, self.user, self.password, self.database, charset='utf8', port=self.port) self.cursor = self.db.cursor() def process_item(self, item,spider): print(item['title']) data = dict(item) sql = "insert into %s values ('%s','%s','%s','%s','%s','%s')" % (self.table,data['title'], data['price'], data['shop'], data['image'], data['deal'], data['location']) try: # 执行sql语句 self.cursor.execute(sql) # 提交到数据库执行 self.db.commit() return item except: # 如果发生错误则回滚 self.db.rollback() def close_spider(self, spider): self.db.close() 数据处理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import pymysqlimport refrom pyecharts import Bar,Pie# 打开数据库连接db=pymysql.connect(host='localhost',user='root',password='123456',port=3306,db='scrapytaobao')# 使用cursor()方法获取操作游标cursor = db.cursor()sum=['huawei','apple','samsung','xiaomi','oppo','vivo']# SQL 查询语句#sql = "SELECT * FROM EMPLOYEE WHERE INCOME &gt; %s" % (1000)v1=[]v2=[]v3=[]v4=[]v5=[]v6=[]v7=[]for sq in sum: sql = "select * from %s"% sq count=0 #总销量 product = &#123;'5-10k': 0, '10-20k': 0, '20-30k': 0, '30-40k': 0, '40-50k': 0, '50-60k': 0, '60k以上': 0&#125; print(sq) try: # 执行SQL语句 cursor.execute(sql) # 获取所有记录列表 results = cursor.fetchall() for row in results: price = re.search('.*?(\d+)',row[1])#价格 price=price.group(1) if '万'in row[4]: #月销量 deal = re.search('(.*?)万', row[4]) deal = float(deal.group(1)) deal=int(deal*10000) else: deal = re.search('(\d+)', row[4]) deal = int(deal.group(1)) price=int(price) count = count + deal if 500&lt;price and price&lt;1000: product['5-10k'] = product['5-10k'] + deal if 1001&lt;price and price&lt;2000: product['10-20k'] = product['10-20k'] + deal if 2001&lt;price and price&lt;3000: product['20-30k'] = product['20-30k'] + deal if 3001&lt;price and price&lt;4000: product['30-40k'] = product['30-40k'] + deal if 4001&lt;price and price&lt;5000: product['40-50k'] = product['40-50k'] + deal if 5001&lt;price and price&lt;6000: product['50-60k'] = product['50-60k'] + deal if 6001&lt;price: product['60k以上'] = product['60k以上'] + deal except: print ("Error: unable to fecth data") print(product) print(count) v1.append(product['5-10k']) v2.append(product['10-20k']) v3.append(product['20-30k']) v4.append(product['30-40k']) v5.append(product['40-50k']) v6.append(product['50-60k']) v7.append(product['60k以上']) bar = Bar("价格-销量图") bar.add(sq,list(product.keys()),list(product.values()), is_label_show=True) sqs=sq+'.html' bar.render(sqs) pie = Pie('各价位销量占比图',sq) pie.add(sq,list(product.keys()),list(product.values()), is_label_show=True) sqs = sq + '1.html' pie.render(sqs)attr =['huawei','apple','samsung','xiaomi','oppo','vivo']bar1=Bar('各品牌手机对比图',height=720)bar1.add('5-10k',attr,v1,is_stack=True)bar1.add('10-20k',attr,v2,is_stack=True)bar1.add('20-30k',attr,v3,is_stack=True)bar1.add('30-40k',attr,v4,is_stack=True)bar1.add('40-50k',attr,v5,is_stack=True)bar1.add('50-60k',attr,v6,is_stack=True)bar1.add('60k以上',attr,v7,is_stack=True)bar1.render('各品牌手机对比图.html')# 关闭数据库连接db.close() 数据可视化华为手机 苹果手机 三星手机 小米手机 vivo手机 oppo手机 各品牌手机对比图 问题问题1：淘宝需要登陆，而且用selenium模拟登陆自动输入账号密码自动滑动滑块也无法登陆，只能手动用手机淘宝扫描二维码进行登陆。问题2：本来我是想进入每个商品的详情页进行爬取，但发现详情页的重要内容比如价格销量在浏览器上可以看到但无论是直接爬下来还是模拟浏览器爬都无法爬取，所以说淘宝的反爬措施很强]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于深度学习的多维智能车载人机交互控制系统]]></title>
    <url>%2F2019%2F04%2F24%2F%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%A4%9A%E7%BB%B4%E6%99%BA%E8%83%BD%E8%BD%A6%E8%BD%BD%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[注：这是本人参加挑战杯负责的项目部分，这次的手势识别方案与电设时的手势识别方案不同，相比之下这次的识别更加准确稳定受光线影响小进步明显，由于代码过长（1000多行）不方便展示，所以这里只展示核心部分代码 设计思路：作品设计分为三大部分，分别是手势语音识别平台、车载控制系统、HUD和触摸屏显示系统。手势语音识别平台负责采集并识别人对车辆发出的交互指令，然后对从机（即控制系统）进行数据传送。车载系统负责接收手势语音识别平台发送过来的指令信息，实现对汽车的中控屏、HUD、车灯、雨刮、接听电话、调节音量等功能的控制；同时还能收集车辆运行时的参数（如车速，油量等）在HUD系统上显示出来。HUD和触摸屏显示系统分为HUD显示器和15寸触摸屏，HUD显示器通过投影技术把车辆仪表盘，导航路线投影到前窗玻璃上；触摸屏主要用以显示车载的软件系统、音乐音量、来电显示等。 创新点：运用手势和语音这两种更自然、更直观的交互手段，以及目前日趋成熟的触摸屏进行多维的控制，给驾驶员带来与众不同的驾驶以及人车交互的体验，增加了行车时操作车载系统的便利性，避免驾驶员注意力分散，从而保证行车安全。 技术关键：手势识别技术、语音识别交互技术、图像处理技术，嵌入式控制技术，HUD显示技术、处理软件的自主研发。 项目设计方案：手势识别：使用Google开源深度学习框架tensorflow搭建CNN神经网络训练手势识别模型对手势进行时时动态识别，目前可以识别8种手势语音识别：调用科大讯飞语音识别API对语音信息进行时时识别和回复车载系统界面：使用Python的图形界面设计库PYQT5设计车载系统界面 核心代码实现手势识别部分代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221def main(self): global joint_detections os.environ['CUDA_VISIBLE_DEVICES'] = str(FLAGS.gpu_id) """ Initial tracker最初的追踪 """ tracker = tracking_module.SelfTracker([FLAGS.webcam_height, FLAGS.webcam_width], FLAGS.input_size) """ Build network graph构建网络图 """ model = cpm_model.CPM_Model(input_size=FLAGS.input_size, heatmap_size=FLAGS.heatmap_size, stages=FLAGS.cpm_stages, joints=FLAGS.num_of_joints, img_type=FLAGS.color_channel, is_training=False) saver = tf.train.Saver() """ Get output node得到输出节点 """ output_node = tf.get_default_graph().get_tensor_by_name(name=FLAGS.output_node_names) device_count = &#123;'GPU': 1&#125; if FLAGS.use_gpu else &#123;'GPU': 0&#125; sess_config = tf.ConfigProto(device_count=device_count) sess_config.gpu_options.per_process_gpu_memory_fraction = 0.2 sess_config.gpu_options.allow_growth = True sess_config.allow_soft_placement = True with tf.Session(config=sess_config) as sess: model_path_suffix = os.path.join(FLAGS.network_def, 'input_&#123;&#125;_output_&#123;&#125;'.format(FLAGS.input_size, FLAGS.heatmap_size), 'joints_&#123;&#125;'.format(FLAGS.num_of_joints), 'stages_&#123;&#125;'.format(FLAGS.cpm_stages), 'init_&#123;&#125;_rate_&#123;&#125;_step_&#123;&#125;'.format(FLAGS.init_lr, FLAGS.lr_decay_rate, FLAGS.lr_decay_step) ) model_save_dir = os.path.join('models', 'weights', model_path_suffix) print('Load model from [&#123;&#125;]'.format(os.path.join(model_save_dir, FLAGS.model_path))) if FLAGS.model_path.endswith('pkl'): model.load_weights_from_file(FLAGS.model_path, sess, False) else: saver.restore(sess, 'cpm_hand_tf/cpm_hand') # Check weights检查重量 for variable in tf.global_variables(): with tf.variable_scope('', reuse=True): var = tf.get_variable(variable.name.split(':0')[0]) print(variable.name, np.mean(sess.run(var))) # Create webcam instance创建远程实例 if FLAGS.DEMO_TYPE in ['MULTI', 'SINGLE', 'Joint_HM']: cam = cv2.VideoCapture(1) # Create kalman filters建立卡尔曼滤波器 if FLAGS.use_kalman: kalman_filter_array = [cv2.KalmanFilter(4, 2) for _ in range(FLAGS.num_of_joints)] for _, joint_kalman_filter in enumerate(kalman_filter_array): joint_kalman_filter.transitionMatrix = np.array( [[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32) joint_kalman_filter.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32) joint_kalman_filter.processNoiseCov = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32) * FLAGS.kalman_noise else: kalman_filter_array = None if FLAGS.DEMO_TYPE.endswith(('png', 'jpg')): test_img = cpm_utils.read_image(FLAGS.DEMO_TYPE, [], FLAGS.input_size, 'IMAGE') test_img_resize = cv2.resize(test_img, (FLAGS.input_size, FLAGS.input_size)) test_img_input = self.normalize_and_centralize_img(test_img_resize) t1 = time.time() predict_heatmap, stage_heatmap_np = sess.run([model.current_heatmap, output_node, ], feed_dict=&#123;model.input_images: test_img_input&#125; ) print('fps: %.2f' % (1 / (time.time() - t1))) self.correct_and_draw_hand(test_img, cv2.resize(stage_heatmap_np[0], (FLAGS.input_size, FLAGS.input_size)), kalman_filter_array, tracker, tracker.input_crop_ratio, test_img) # Show visualized image显示可视化图像 # demo_img = visualize_result(test_img, stage_heatmap_np, kalman_filter_array) #cv2.imshow('demo_img', test_img.astype(np.uint8)) #cv2.waitKey(0) elif FLAGS.DEMO_TYPE in ['SINGLE', 'MULTI']: while True: # # Prepare input image准备输入图像 _, full_img = cam.read() test_img = tracker.tracking_by_joints(full_img, joint_detections=joint_detections) crop_full_scale = tracker.input_crop_ratio test_img_copy = test_img.copy() # White balance白色平衡 test_img_wb = utils.img_white_balance(test_img, 5) test_img_input = self.normalize_and_centralize_img(test_img_wb) # Inference推理 t1 = time.time() stage_heatmap_np = sess.run([output_node], feed_dict=&#123;model.input_images: test_img_input&#125;) print('FPS: %.2f' % (1 / (time.time() - t1))) local_img = self.visualize_result(full_img, stage_heatmap_np, kalman_filter_array, tracker, crop_full_scale, test_img_copy) cv2.imshow('global_img', full_img.astype(np.uint8)) #cv2.imshow('local_img', local_img.astype(np.uint8)) #cv2.imwrite('image/' + 'img.jpg', local_img.astype(np.uint8), [int(cv2.IMWRITE_JPEG_QUALITY), 90]) if cv2.waitKey(1) == ord('q'): cam.release() break elif FLAGS.DEMO_TYPE == 'Joint_HM': while True: # Prepare input image准备输入图像 test_img = cpm_utils.read_image([], cam, FLAGS.input_size, 'WEBCAM') test_img_resize = cv2.resize(test_img, (FLAGS.input_size, FLAGS.input_size)) test_img_input = self.normalize_and_centralize_img(test_img_resize) # Inference t1 = time.time() stage_heatmap_np = sess.run([output_node], feed_dict=&#123;model.input_images: test_img_input&#125;) print('FPS: %.2f' % (1 / (time.time() - t1))) demo_stage_heatmap = stage_heatmap_np[len(stage_heatmap_np) - 1][0, :, :, 0:FLAGS.num_of_joints].reshape( (FLAGS.heatmap_size, FLAGS.heatmap_size, FLAGS.num_of_joints)) demo_stage_heatmap = cv2.resize(demo_stage_heatmap, (FLAGS.input_size, FLAGS.input_size)) vertical_imgs = [] tmp_img = None joint_coord_set = np.zeros((FLAGS.num_of_joints, 2)) for joint_num in range(FLAGS.num_of_joints): # Concat until 4 img直到凌晨4点 if (joint_num % 4) == 0 and joint_num != 0: vertical_imgs.append(tmp_img) tmp_img = None demo_stage_heatmap[:, :, joint_num] *= (255 / np.max(demo_stage_heatmap[:, :, joint_num])) # Plot color joints图颜色关节 if np.min(demo_stage_heatmap[:, :, joint_num]) &gt; -50: joint_coord = np.unravel_index(np.argmax(demo_stage_heatmap[:, :, joint_num]), (FLAGS.input_size, FLAGS.input_size)) joint_coord_set[joint_num, :] = joint_coord color_code_num = (joint_num // 4) if joint_num in [0, 4, 8, 12, 16]: joint_color = list( map(lambda x: x + 35 * (joint_num % 4), FLAGS.joint_color_code[color_code_num])) cv2.circle(test_img, center=(joint_coord[1], joint_coord[0]), radius=3, color=joint_color, thickness=-1) else: joint_color = list( map(lambda x: x + 35 * (joint_num % 4), FLAGS.joint_color_code[color_code_num])) cv2.circle(test_img, center=(joint_coord[1], joint_coord[0]), radius=3, color=joint_color, thickness=-1) # Put text把文本 tmp = demo_stage_heatmap[:, :, joint_num].astype(np.uint8) tmp = cv2.putText(tmp, 'Min:' + str(np.min(demo_stage_heatmap[:, :, joint_num])), org=(5, 20), fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=0.3, color=150) tmp = cv2.putText(tmp, 'Mean:' + str(np.mean(demo_stage_heatmap[:, :, joint_num])), org=(5, 30), fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=0.3, color=150) tmp_img = np.concatenate((tmp_img, tmp), axis=0) \ if tmp_img is not None else tmp # Plot FLAGS.limbs情节FLAGS.limbs for limb_num in range(len(FLAGS.limbs)): if np.min(demo_stage_heatmap[:, :, FLAGS.limbs[limb_num][0]]) &gt; -2000 and np.min( demo_stage_heatmap[:, :, FLAGS.limbs[limb_num][1]]) &gt; -2000: x1 = joint_coord_set[FLAGS.limbs[limb_num][0], 0] y1 = joint_coord_set[FLAGS.limbs[limb_num][0], 1] x2 = joint_coord_set[FLAGS.limbs[limb_num][1], 0] y2 = joint_coord_set[FLAGS.limbs[limb_num][1], 1] length = ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5 if length &lt; 10000 and length &gt; 5: deg = math.degrees(math.atan2(x1 - x2, y1 - y2)) polygon = cv2.ellipse2Poly((int((y1 + y2) / 2), int((x1 + x2) / 2)), (int(length / 2), 3), int(deg), 0, 360, 1) color_code_num = limb_num // 4 limb_color = list( map(lambda x: x + 35 * (limb_num % 4), FLAGS.joint_color_code[color_code_num])) cv2.fillConvexPoly(test_img, polygon, color=limb_color) if tmp_img is not None: tmp_img = np.lib.pad(tmp_img, ((0, vertical_imgs[0].shape[0] - tmp_img.shape[0]), (0, 0)), 'constant', constant_values=(0, 0)) vertical_imgs.append(tmp_img) # Concat horizontally Concat水平 output_img = None for col in range(len(vertical_imgs)): output_img = np.concatenate((output_img, vertical_imgs[col]), axis=1) if output_img is not None else \ vertical_imgs[col] output_img = output_img.astype(np.uint8) output_img = cv2.applyColorMap(output_img, cv2.COLORMAP_JET) test_img = cv2.resize(test_img, (300, 300), cv2.INTER_LANCZOS4) #cv2.imshow('hm', output_img) #cv2.moveWindow('hm', 2000, 200) #cv2.imshow('rgb', test_img) #cv2.moveWindow('rgb', 2000, 750) if cv2.waitKey(1) == ord('q'): break 语音部分代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122class Msp: def __init__(self): pass def read(self): global filename, dll, login_params # 音频文件路径 input_filename = "output.wav" input_filepath = "D:/大挑项目有语音回复版本/5c5451a0/bin/audio/" in_path = input_filepath + input_filename # 块大小、每次采集的位数、声道数、采样率：每秒采集数据的次数、录音时间、文件存放位置 CHUNK = 256 # 1024 FORMAT = pyaudio.paInt16 CHANNELS = 1 # 2 RATE = 11025 # 44100 RECORD_SECONDS = 5 WAVE_OUTPUT_FILENAME = in_path # 开始录音 p = pyaudio.PyAudio() stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK) print("* recording *") frames = [] for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)): data = stream.read(CHUNK) frames.append(data) print("* done recording *") # 结束录音 stream.stop_stream() stream.close() p.terminate() wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb') wf.setnchannels(CHANNELS) wf.setsampwidth(p.get_sample_size(FORMAT)) wf.setframerate(RATE) wf.writeframes(b''.join(frames)) wf.close() # 调用动态链接库 dll = cdll.LoadLibrary("D:/大挑项目有语音回复版本/5c5451a0/bin/msc_x64.dll") # 登录参数，apppid一定要和你的下载SDK对应，work_dir是SDK文件夹名 filename = in_path # 初始化msc，用户登录 def login(self): ret = dll.MSPLogin(None, None, login_params) # print('MSPLogin =&gt;', ret) # 退出登录 def logout(self): ret = dll.MSPLogout() # print('MSPLogout =&gt;', ret) def isr(self, audiofile, session_begin_params): ret = c_int() sessionID = c_voidp() dll.QISRSessionBegin.restype = c_char_p # 开始一次语音识别 sessionID = dll.QISRSessionBegin(None, session_begin_params, byref(ret)) # print('QISRSessionBegin =&gt; sessionID:', sessionID, 'ret:', ret.value) # 每秒【1000ms】 1600 次 * 16 bit 【2B】 ，每毫秒：1.6 * 16bit 【1.6*2B】 = 32Byte # 1帧音频20ms【640B】 每次写入 10帧=200ms 【6400B】 piceLne = FRAME_LEN * 20 piceLne = 1638 * 2 epStatus = c_int(0) recogStatus = c_int(0) wavFile = open(audiofile, 'rb') wavData = wavFile.read(piceLne) # 本接口需不断调用，直到音频全部写入为止。上传音频时，需更新audioStatus的值。具体来说: # 当写入首块音频时, 将audioStatus置为MSP_AUDIO_SAMPLE_FIRST # 当写入最后一块音频时, 将audioStatus置为MSP_AUDIO_SAMPLE_LAST # 其余情况下, 将audioStatus置为MSP_AUDIO_SAMPLE_CONTINUE # 同时，需定时检查两个变量：epStatus和rsltStatus。具体来说: # 当epStatus显示已检测到后端点时，MSC已不再接收音频，应及时停止音频写入 # 当rsltStatus显示有识别结果返回时，即可从MSC缓存中获取结果 # 写入本次识别的音频 ret = dll.QISRAudioWrite(sessionID, wavData, len(wavData), MSP_AUDIO_SAMPLE_FIRST, byref(epStatus), byref(recogStatus)) # print('len(wavData):', len(wavData), 'QISRAudioWrite ret:', ret, 'epStatus:', epStatus.value, 'recogStatus:', # recogStatus.value) time.sleep(0.05) while wavData: wavData = wavFile.read(piceLne) if len(wavData) == 0: break ret = dll.QISRAudioWrite(sessionID, wavData, len(wavData), MSP_AUDIO_SAMPLE_CONTINUE, byref(epStatus), byref(recogStatus)) # print('len(wavData):', len(wavData), 'QISRAudioWrite ret:', ret, 'epStatus:', epStatus.value, # 'recogStatus:', recogStatus.value) time.sleep(0.05) wavFile.close() ret = dll.QISRAudioWrite(sessionID, None, 0, MSP_AUDIO_SAMPLE_LAST, byref(epStatus), byref(recogStatus)) # print('len(wavData):', len(wavData), 'QISRAudioWrite ret:', ret, 'epStatus:', epStatus.value, 'recogStatus:', # recogStatus.value) print("所有待识别音频已全部发送完毕，等待获取识别结果") # 获取音频 laststr = '' counter = 0 while recogStatus.value != MSP_REC_STATUS_COMPLETE: ret = c_int() # 获取识别结果 dll.QISRGetResult.restype = c_char_p retstr = dll.QISRGetResult(sessionID, byref(recogStatus), 0, byref(ret)) if retstr is not None: laststr += retstr.decode() #print(laststr) # print('ret:', ret.value, 'recogStatus:', recogStatus.value) counter += 1 #time.sleep(0.2) counter += 1 if counter == 500: laststr += '讯飞语音识别失败' break # print(laststr) # 结束本次语音识别 ret = dll.QISRSessionEnd(sessionID, 'END') # print('end ret: ', ret) return laststr 车载系统界面部分代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113class Desktop(QMainWindow): def __init__(self): super().__init__() self.init_ui() """对界面进行布局和组件的布置""" def init_ui(self): self.main_widget = QtWidgets.QWidget() # 创建窗口主部件 self.main_layout = QtWidgets.QGridLayout() # 创建主部件的网格布局 self.main_widget.setLayout(self.main_layout) # 设置窗口主部件布局为网格布局 self.window = QtWidgets.QWidget() # 创建显示窗口 self.window.setObjectName('window') self.browser = QWebEngineView() self.browser.load(QUrl('https://map.baidu.com/')) QApplication.processEvents() self.setCentralWidget(self.browser) self.child0=Msp() self.child1 = Example1() self.child2 = Example2() self.child3 = Example3() self.click() def click(self): self.taskbur = QtWidgets.QWidget() # 创建任务栏 self.taskbur.setObjectName('taskbur') self.taskbur_layout = QtWidgets.QHBoxLayout() # 创建左侧部件的水平布局层 self.taskbur.setLayout(self.taskbur_layout) # 设置左侧部件布局为水平 self.taskbur.setStyleSheet('QWidget&#123;background-color:rgb(16, 15, 18)&#125;') # 设置背景色 self.main_layout.addWidget(self.browser, 0, 0, 30, 12) # 任务栏部件在第0行第0列，占30行12列 self.main_layout.addWidget(self.taskbur, 31, 0, 2, 12) # 显示窗口部件在第31行第0列，占2行12列 self.setCentralWidget(self.main_widget) # 设置窗口主部件 """任务栏布置""" # 控件QPushButton的定义和设置 self.btn1 = QPushButton(self) self.btn1.setMaximumSize(73, 50) # 图片分辨率为219x150 self.btn1.setMinimumSize(73, 50) self.btn1.setStyleSheet("QPushButton&#123;border-image: url(D:/大挑项目有语音回复版本/Icon/model.png)&#125;") ### self.taskbur_layout.addWidget(self.btn1) self.btn1.clicked.connect(self.Work0) self.btn2 = QPushButton(self) self.btn2.setMaximumSize(81, 80) # 图片分辨率为126x125 self.btn2.setMinimumSize(81, 80) self.btn2.setStyleSheet("QPushButton&#123;border-image: url(D:/大挑项目有语音回复版本/Icon/音乐.png)&#125;") self.taskbur_layout.addWidget(self.btn2) self.btn2.clicked.connect(self.Work1) self.btn3 = QPushButton(self) self.btn3.setMaximumSize(70, 70) # 图片分辨率为152x152 self.btn3.setMinimumSize(70, 70) self.btn3.setStyleSheet("QPushButton&#123;border-image: url(D:/大挑项目有语音回复版本/Icon/加热.png)&#125;") self.taskbur_layout.addWidget(self.btn3) self.btn3.clicked.connect(self.Work8) self.btn4 = QPushButton(self) self.btn4.setMaximumSize(70, 70) # 图片分辨率为219x150 self.btn4.setMinimumSize(70, 70) self.btn4.setStyleSheet("QPushButton&#123;border-image: url(D:/大挑项目有语音回复版本/Icon/左座椅.png)&#125;") self.taskbur_layout.addWidget(self.btn4) self.btn4.clicked.connect(self.Work5) self.btn5 = QPushButton(self) self.btn5.setMaximumSize(70, 70) # 图片分辨率为200x200 self.btn5.setMinimumSize(70, 70) self.btn5.setStyleSheet("QPushButton&#123;border-image: url(D:/大挑项目有语音回复版本/Icon/空调开放.png)&#125;") self.taskbur_layout.addWidget(self.btn5) self.btn5.clicked.connect(self.Work2) self.btn6 = QPushButton(self) self.btn6.setMaximumSize(70, 70) # 图片分辨率为184x184 self.btn6.setMinimumSize(70, 70) self.btn6.setStyleSheet("QPushButton&#123;border-image: url(D:/大挑项目有语音回复版本/Icon/右座椅.png)&#125;") self.taskbur_layout.addWidget(self.btn6) self.btn6.clicked.connect(self.Work7) self.btn7 = QPushButton(self) self.btn7.setMaximumSize(70, 70) # 图片分辨率为123x123 self.btn7.setMinimumSize(70, 70) self.btn7.setStyleSheet("QPushButton&#123;border-image: url(D:/大挑项目有语音回复版本/Icon/除霜.png)&#125;") self.taskbur_layout.addWidget(self.btn7) self.btn7.clicked.connect(self.Work6) self.btn8 = QPushButton(self) self.btn8.setMaximumSize(71, 70) # 图片分辨率为145x144 self.btn8.setMinimumSize(71, 70) self.btn8.setStyleSheet("QPushButton&#123;border-image: url(D:/大挑项目有语音回复版本/Icon/电话.png)&#125;") self.taskbur_layout.addWidget(self.btn8) self.btn8.clicked.connect(self.Work3) self.btn9 = QPushButton(self) self.btn9.setMaximumSize(70, 70) # 图片分辨率为124x124 self.btn9.setMinimumSize(70, 70) self.btn9.setStyleSheet("QPushButton&#123;border-image: url(D:/大挑项目有语音回复版本/Icon/音量.png)&#125;") self.taskbur_layout.addWidget(self.btn9) self.btn9.clicked.connect(self.Work4) """显示窗口布置""" self.btn10 = QPushButton(self) self.btn10.setGeometry(480, 10, 60, 60) # 图片分辨率为130x130 self.btn10.setStyleSheet("QPushButton&#123;border-image: url(D:/大挑项目有语音回复版本/Icon/特斯拉logo.png)&#125;") self.btn11 = QPushButton(self) self.btn11.setGeometry(80, 10, 60, 60) # 图片分辨率为130x130 self.btn11.setStyleSheet("QPushButton&#123;border-image: url(D:/大挑项目有语音回复版本/Icon/4G.png)&#125;") self.btn12 = QPushButton(self) self.btn12.setGeometry(40, 10, 60, 60) # 图片分辨率为130x130 self.btn12.setStyleSheet("QPushButton&#123;border-image: url(D:/大挑项目有语音回复版本/Icon/蓝牙.png)&#125;") 最终实现效果打开音乐 控制音量 打开通话界面 返回地图 打开语音 其他还有打开车灯，空调等手势]]></content>
      <categories>
        <category>智能</category>
      </categories>
      <tags>
        <tag>智能</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于OpenCV和TensorFlow的手势识别控制的人机交互]]></title>
    <url>%2F2019%2F04%2F22%2F%E5%9F%BA%E4%BA%8EOpenCV%E5%92%8CTensorFlow%E7%9A%84%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB%E6%8E%A7%E5%88%B6%E7%9A%84%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92%2F</url>
    <content type="text"><![CDATA[注：这是本人参加省电设负责的项目部分，主要负责手势识别 功能与指标：1.通过摄像头捕捉用户手势，对手势进行识别（目前可以识别7种手势），将手势变化信息进行处理，并发送指令到单片机上，进而实现模拟鼠标控制操作。2.手势空间位置的移动模拟计算机鼠标的移动，当手势变化时，可模拟计算机鼠标的左键单击、左键双击、右键单击、滑轮移动、最大化最小化快捷键、语音快捷键和报警功能。3.该作品除了实现计算机的鼠标功能，还实现某些智能产品的部分功能，当打开语音功能时，用户可通过语音控制计算机。4.当打开报警功能后，摄像头转变为监控摄像头，当摄像头监视到物体运动则会将信息反馈到用户手机并开始录制视频，用户可查看监控内容，起到防火，防盗，防烟雾等功能。 实现原理：大量收集各种手型不同手势在移动背景下的视频，将视频按帧数导出图片，加入负样本，再使用OpenCV提取手的LBP和AdaBoost特征进行机器学习训练出LBP+AdaBoost人手检测模型， 即得到xml训练集。打开外接摄像头，将摄像头捕捉的视频流中每一帧图像进行处理定位图像中的手势，再传给用TensorFlow进行深度学习训练好的手势识别模型文件进行快速识别。设定一个手势为移动手势，当识别到该手势时只做单一的识别即只识别手势移动信息，当手势为非移动手势且存在变化后的手势超过两帧时，会视为手势变化，遍历所有手势的训练集，快速找出与其相匹配的手势，并下达一次相应的指令，避免重复下达同一指令，而且从下达指令到切换成移动手势的这段时间内锁上其他非移动指令（即识别一种非移动手势变化后必须以移动手势作为重新识别另一种非移动手势的开始），避免手势变化时的指令冲突。将识别到手势信息及位移信息通过串口传给单片机，然后单片机根据USB协议进行鼠标及快捷键操作。另外当识别到报警功能的手势时，10秒后报警功能开启，摄像头只识别报警手势，转变为监控摄像头，进行运动检测，将捕捉的后一帧与前一帧进行对照，当摄像头前出现运动或光线发生激烈变化时表明有物体在移动就会产生报警信息 训练训练的图片各2500 训练的精确度 实现程序：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278#注意本程序在非GPU版本的tensorflow是正常运行的，在GPU版本的tensorflow就会出错import tensorflow as tfimport osfrom PIL import Imageimport matplotlib.pyplot as pltimport numpy as npimport datetimeimport timeimport cv2import serial#pil=serial.Serial('COM4',9600)Time=datetime.datetime.now()IS='%Y-%m-%d %H:%M:%S'Time=datetime.datetime.now().strftime(IS)font=cv2.FONT_HERSHEY_SIMPLEX# 1.常见一个BackgroundSubtractorKNN接口bs = cv2.createBackgroundSubtractorMOG2()cap = cv2.VideoCapture(0)#'666.avi'0'hand.mp4'fps = 25size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))vidw = cv2.VideoWriter('C:/Users/admin/Desktop/sp/sp3.avi', cv2.VideoWriter_fourcc('I', '4', '2', '0'), fps, size)kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))fiste = cv2.CascadeClassifier('csdn/fist.xml')z=0#用来判断fist是否被识别到m='a'x1=0x2=0x3=0x4=0y1=0 y2=0y3=0y4=0g=0o=0#用来防止重复识别，保证fist再次被识别后才能再进行一次识别a=0#用来确定fistb不被识别进行手势识别b=0#b是用来防止偶尔环境产生的识别lines=tf.gfile.GFile('D:/电设/dsbc/retrain1/output_labels.txt').readlines()#注意不要把readlines()错打成readline()，前者读一整行后者读一个字母uid_to_humae=&#123;&#125;#一行一行读取数据for uid,line in enumerate(lines): #去掉换行符 line=line.strip('\n') uid_to_humae[uid]=linedef id_to_string(node_id): if node_id not in uid_to_humae: return '' return uid_to_humae[node_id]# 创建一个图来存放google训练好的模型with tf.gfile.FastGFile('D:/电设/dsbc/retrain1/output_graph.pb', 'rb') as f:# 二进制读取模型文件 graph_def = tf.GraphDef()# 新建GraphDef文件，用于临时载入模型中的图 graph_def.ParseFromString(f.read())# GraphDef加载模型中的图 tf.import_graph_def(graph_def, name='')# 在空白图中加载GraphDef中的图root='D:/电设/dsbc/'file='image4.jpg''''if pil.isOpen()==False:#确定串口是否打开 pil.open()'''with tf.Session() as sess: softmax_tensor = sess.graph.get_tensor_by_name('final_result:0') # 给图起个名字，sess.graph.get_tensor_by_name根据名称返回tensor数据 https://blog.csdn.net/lenbow/article/details/52181159 while (cap.isOpened()): ret, img = cap.read() if ret == True: gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) divisor = 16 hi, wi = np.shape(img)[:2] minSize = (int(wi / divisor), int(hi / divisor)) fists = fiste.detectMultiScale(gray, 1.3, 5, cv2.CASCADE_SCALE_IMAGE, minSize) # minSize和maxSize用来限制得到的目标区域的范围。如果视频中误检到很多无用的小方框，那么就把minSize的尺寸改大一些，默认的为30*30 if len(fists) &gt; 0: b=b+1 #b是用来防止偶尔环境产生的识别 x, y, w, h = fists[0] s=int(164/w*100)#根据像素大小判断手距离给鼠标加速（乗100是精确到小数点后两位） xi = int((2 * x + w) / 2) yi = int((2 * y + h) / 2) #cv2.rectangle(img, (x, y), (x + w, y + h), color1, 2) if z == 1: xz0 = xi - x1 yz0 = yi - y1 xz1 = x1 - x2 xz2 = x2 - x3 xz3 = x3 - x4 yz1 = y1 - y2 yz2 = y2 - y3 yz3 = y3 - y2 xz = int(int((xz0 + xz1 + xz2+xz3) / 3)*s/100) yz = int(int((yz0 + yz1 + yz2+yz3) / 3)*s/100) if abs(xz) + abs(yz) &lt; 60: if xz &gt;= 0: if xz &lt; 10: xz = str(xz) + 'o' else: xz = str(xz) xz = '+' + xz else: if -xz &lt; 10: xz = str(xz) + 'o' else: xz = str(xz) if yz &gt;= 0: if yz &lt; 10: yz = str(yz) + 'o' else: yz = str(yz) yz = '+' + yz else: if -yz &lt; 10: yz = str(yz) + 'o' else: yz = str(yz) vec = m + xz + m + yz + '\n' print(vec) #pil.write(bytes(vec, 'utf-8')) x4 = x3 x3 = x2 x2 = x1 x1 = xi y4 = y3 y3 = y2 y2 = y1 y1 = yi a = 0 z = 1 #用来判断fist是否被识别到 if b==4: o=0 else: z = 0 vec = 'oooooooo' + '\n' print(vec) #pil.write(bytes(vec, 'utf-8')) b = 0 if z == 0 and x1!= 0 and y1!= 0 and o == 0: a = a + 1#用来确定fistb不被识别进行手势识别 if a==4: #time.sleep(1) ret, frame= cap.read() ox=int(4*w/5) oy=int(4*h/5) ix = int(1* w / 4) iy = int(1* h / 4) x=x-ox if x&lt;0: x=0 y=y-oy if y&lt;0: y=0 w=2*w+ix h=2*h+iy imgi = frame[y:y+h,x:x+w] cv2.imwrite(file, imgi, [cv2.IMWRITE_JPEG_CHROMA_QUALITY, 100]) image_data = tf.gfile.FastGFile(os.path.join(root, file), 'rb').read()#读取图像 predictions = sess.run(softmax_tensor, &#123;'DecodeJpeg/contents:0': image_data&#125;) # 用上面定义的方法softmax_tensor计算图片，'DecodeJpeg/contents:0'指明图片格式是jpg格式，得到二维数据predictions predictions = np.squeeze(predictions) #把结果转为1维数据 np.squeeze() 可以直接进行压缩维度 # 打印图片路径及名称 image_path = os.path.join(root, file) print(image_path) # 显示图片 img = Image.open(image_path) #plt.imshow(img) #plt.axis('off') # 排序，因为有1000个结果，先按概率大小从小到大排，再取后面5个[-5],再对值做个倒序[::-1] top_k = predictions.argsort()[::-1] # argsort函数返回的是数组值从小到大的索引值 node_id=top_k[0] # 获取分类名称 human_string = id_to_string(node_id) # 获取该分类的置信度 score = predictions[node_id] print('%s (score = %.5f)' % (human_string, score)) #plt.show() o=1#用来防止重复识别，保证fist再次被识别后才能再进行一次识别 a=0 if human_string == 'b': print('b') vec = 'b' + '\n' #pil.write(bytes(vec, 'utf-8')) time.sleep(0.5) if human_string == 'c': print('c') vec = 'c' + '\n' #pil.write(bytes(vec, 'utf-8')) time.sleep(0.5) if human_string == 'g': print('d') vec = 'd' + '\n' #pil.write(bytes(vec, 'utf-8')) time.sleep(0.5) if human_string == 'e': if m=='e': m='a' else: m='e' if human_string == 'f': print('h') vec = 'h' + '\n' #pil.write(bytes(vec, 'utf-8')) time.sleep(0.5) if human_string == 'o': print('d') vec = 'd' + '\n' #pil.write(bytes(vec, 'utf-8')) time.sleep(0.5) if human_string == 's': if g==0: print('f') vec = 'f' + '\n' #pil.write(bytes(vec, 'utf-8')) time.sleep(0.5) if human_string == 'j': time.sleep(10) j = 0 k=0 while True: Time = datetime.datetime.now().strftime(IS) ret, frame = cap.read() # 3. apply()函数计算了前景掩码 fgmask = bs.apply(frame) # 4. 获得前景掩码（含有白色值以及阴影的灰色值），通过设定阈值将非白色（244~255）的所有像素都设为0，而不是1； th = cv2.threshold(fgmask.copy(), 244, 255, cv2.THRESH_BINARY)[1] # 二值化操作，[1]即取第二个返回值（0为第一个） dilated = cv2.dilate(th, kernel, iterations=2) # 5.膨胀操作 dilated = cv2.erode(th, kernel, iterations=2) # cv2.getStructuringElement 构建一个椭圆形的核 # 3x3卷积核中有2个1那就设置为1 # 6. findContours函数参数说明cv2.RETR_EXTERNAL只检测外轮廓， # cv2.CHAIN_APPROX_SIMPLE只存储水平，垂直，对角直线的起始点。 image, contours, hier = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # 查找轮廓 if len(contours) &gt; 0: cv2.putText(frame, Time, (10, 30), font, 1, (0, 0, 255), 2, cv2.LINE_AA) vidw.write(frame) if j == 0: print('j') vec = 'j' + '\n' #pil.write(bytes(vec, 'utf-8')) time.sleep(1) print('ok') j = 1 gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) fists = fiste.detectMultiScale(gray, 1.3, 5, cv2.CASCADE_SCALE_IMAGE, minSize) # minSize和maxSize用来限制得到的目标区域的范围。如果视频中误检到很多无用的小方框，那么就把minSize的尺寸改大一些，默认的为30*30 if len(fists) &gt; 0: if k == 3:#用于确定要退出报警循环 o = 0 k = 0 print('j') vec = 'j' + '\n' #pil.write(bytes(vec, 'utf-8')) time.sleep(0.5) break k = k + 1 else: if k &lt;3: k= 0 else: if a &lt;4: a = 0]]></content>
      <categories>
        <category>智能</category>
      </categories>
      <tags>
        <tag>智能</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何用Opencv训练手势识别模型]]></title>
    <url>%2F2019%2F04%2F21%2F%E5%A6%82%E4%BD%95%E7%94%A8Opencv%E8%AE%AD%E7%BB%83%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[第一步准备好正负样本图片，正样本就是含有目标的图片，负样本就是不还有目标的图片，这里本人准备了3005张正样本5500张负样本（注意第四步只用来3000张正样本，第五步只用来2500张正样本，而负样本只用来2800张，样本的数量要比样本的使用量大比如会出错或训练中会停在一个地方），建好文件夹，如图所示，data用来保存第四步生成的文件，data1用来保存第五步生成的模型文件，pos和neg用来保存正负样本。其中正样本图片最好是裁剪成同一尺寸，我这里是建成24x24（建议图片大小在20x20到30x30之间）负样本分辨率不作要求大小可以不一，便于后期的说明文件的建立。图片的编辑我使用的是美图看看，批量编辑工具，可以批量将图片改成同一尺寸。 第二步建立正负样本说明文件，在cmd下进入pos文件夹目录里，输入 dir /b &gt; pos.txt,用记事本打开该文件，删除最后一行，最后将名字归一化如下所示：同样的方法进入neg文件夹内，用同样的方法建立描述文件neg.txt，用记事本打开该文件，只需删掉最后一行 第三步打开记事本，去除pos.txt最后一行的pos文件夹；将所有jpg替换成 jpg 1 0 0 24 24。这里1表示当前图片重复出现的次数是1， 0 0 24 24表示目标图片大小是矩形框从（0，0）到（24，24）。注：按自己图片分辨率大小 第四步1opencv_createsamples.exe -vec data/pos.vec -info pos/pos.txt -bg neg/neg.txt -w 24 -h 24 -num 3000 创建vec文件：将opencv自带的opencv_createsamples.exe和 opencv_traincascade.exe放到图片文件夹的上层目录，利用opencv_createsamples.exe应用程序在该目录下使用如图cmd命令：其中的-vec是指定后面输出vec文件的文件名，-info指定正样本描述文件，-bg指定负样本描述文件，-w和-h分别指正样本的宽和高，-num表示正样本的个数。执行完该命令后就会在当前目录下生产一个pos.vec文件了。 第五步使用opencv_traincascade.exe文件进行训练LBP特征首先在当前目录下新建一个data1文件夹用于存放生成的.xml文件。在当前目录使用cmd命令:1opencv_traincascade.exe -data data1 -vec data/pos.vec -bg neg/neg.txt -numPos 2500 -numNeg 2800 -numStages 16 -precalcValbufSize 200 -precalcdxBufSize 1000 -featureType LBP -w 24 -h 24 截图如下：其中-data 输出目录，-numPos正样本数目-numNeg负样本数目-numStages训练级数 在命令行上的操作 训练开始和结束图片 使用opencv_haartraining.exe训练haar特征其他操作一样，以上训练LBP特征的第五步的命令改成如下1opencv_haartraining.exe -vec data/pos.vec -bg neg/neg.txt -data data1 -w 24 -h 24 -mem 1024 -npos 2500 -neg 2800 -nstages 16 -nsplits 5 -data 指定生成的文件目录，-vec vec文件名，-bg 负样本描述文件名称，也就是负样本的说明文件(.dat)-nstage 20 指定训练层数，推荐15~20，层数越高，耗时越长。-nsplits 分裂子节点数目,选取默认值 2-minhitrate 最小命中率，即训练目标准确度。-maxfalsealarm最大虚警(误检率)，每一层训练到这个值小于0.5时训练结束，进入下一层训练，-npos 在每个阶段用来训练的正样本数目，-nneg在每个阶段用来训练的负样本数目 这个值可以设置大于真正的负样本图像数目，程序可以自动从负样本图像中切割出和正样本大小一致的,这个参数一半设置为正样本数目的1~3倍 -w -h样本尺寸，与前面对应 -mem 程序可使用的内存，这个设置为256即可，实际运行时根本就不怎么耗内存，以MB为单位 -mode ALL指定haar特征的种类，BASIC仅仅使用垂直特征，ALL表示使用垂直以及45度旋转特征-sym或者-nonsym，后面不用跟其他参数，用于指定目标对象是否垂直对称，若你的对象是垂直对称的，比如脸，则垂直对称有利于提高训练速度 中间可能遇到的问题与解决问题：问题1：对于正负样本的比率网上有很多种说法，有1：2的有1：3等等，样本的数量说法也不一有的只有几百有的要几千，但我都试过了都会产生下面的问题2，3问题2：可能在训练的过程中，过了很长时间但是却一直停留在某一层不动，上网查找解决办法有的说是样本数量不够有的说要增大负样本数目，增大负样本之间的变化! 增加负样本，然后重新接着训练，注意更改负样本的数目，但我修改了很多次无论是增加还是减小正负样本的数量都不行问题3：由于产生问题2，所以我尝试过减小训练的层数（正常是15到20层之间）但是训练出来的模型完全不能用问题4：负样本不要作处理，只要灰度化就行了大小可以不一 解决：一开始受到以上问题的困扰，网上的解决方法都没用，所以训练了一个星期都没有训练出可以用的模型出来，在一次偶然中我不小心把500张（分辨率为544x960）没有缩小为24x24的正样本图片掺杂在2505张正常的24x24的正样本中居然就训练出来了（这里联系第一步的解说），训练出模型的识别效果还是会受样本数量和正负样本的比率的影响。其实训练一开始就可以判断你这次训练能否成功了，没必要等到最后或中间停止，一般能成功的都是：训练一开始第一层就会有超过8个迭代层（我有一次超过25层）低于5层你就不用等了，而且随着层数的增加迭代层也会增加（会超过30多层或只增加几层），然后再减少到最后一层时一般也能有超过8层。训练过程每一层的训练时间是差不多的，如果随着训练层次的增加训练时间也会增加很多的话有可能也会训练不出来，训练LBP特征会很快一般只要不到10分钟，而训练haar特征时间很长一般要7到8小时，训练出来的xml文件也比LBP（不到100k）大很多，一般几百k。 模型效果]]></content>
      <categories>
        <category>智能</category>
      </categories>
      <tags>
        <tag>智能</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于物联网ONENET平台的温湿度报警器]]></title>
    <url>%2F2019%2F04%2F20%2F%E5%9F%BA%E4%BA%8E%E7%89%A9%E8%81%94%E7%BD%91ONENET%E5%B9%B3%E5%8F%B0%E7%9A%84%E6%B8%A9%E6%B9%BF%E5%BA%A6%E6%8A%A5%E8%AD%A6%E5%99%A8%2F</url>
    <content type="text"><![CDATA[注：这是本人合泰杯负责的项目部分 作品设计构想基于此，本作品使用合泰HT66F70A主控 ，集成采集模块、红外接收模块、讯飞语音模块、WIFI模块、智能云平台（ONENET）。 功能应用本作品由数据采集模块、讯飞语音模块、WIFI模块、智能云平台组成，具有以下设计：①语音：使用讯飞语音模块，通过模拟串口与HT66F70A对接，将相关信息转化为语音，从而实现语音播报。②监测与实时显示功能：外置温湿传感器DHT11、移动物体检测模块对周围环境进行检测，并可通过WIFI模块发送至智能云平台，从而实现三端显示。③异常环境报警（低温、高温、潮湿）：根据传感器相关数据，分析并判断异常环境状态，达到警戒值时，发出报警。 DHT11温湿度传感器和LCD显示屏的控制代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151# include &lt;reg52.h&gt; # include &lt;intrins.h&gt; typedef unsigned char BYTE; typedef unsigned int WORD; #define uint unsigned int #define uchar unsigned char sbit io=P2^0;//dht11data端接单片机的P1^0口// sbit rs=P3^5; sbit ep=P3^4; uchar data_byte; uchar RH,RL,TH,TL; //***************延时函数************************************* void delay0(uchar ms) &#123; uchar i; while(ms--) for(i=0;i&lt;100;i++); &#125; void delay1() // &#123; uchar i; for(i=0;i&lt;1;i++); &#125; void write_cmd(uchar cmd)//写指令// &#123; rs=0; // rw=0; ep=0; P0=cmd; delay0(5); ep=1; delay0(5); ep=0; &#125; void write_addr(uchar addr)//写地址// &#123; write_cmd(addr|0x80);//LCD第一行的首地址为0x800x80+0x40=0xc0 &#125; void write_byte(uchar dat) //写字节// &#123; rs=1; ep=0; P0=dat; delay0(5); ep=1; delay0(5); ep=0; &#125; void lcd_init() //lcd初始化// &#123; ep=0; write_cmd(0x38);//设置LCD5*7点阵表示,数据由8跟线传输 write_cmd(0x0c);//清除屏幕显示 write_cmd(0x06); write_cmd(0x01); &#125; void display(uchar addr,uchar q)//adderq表示显示的字符或数字 &#123; delay0(10); write_addr(addr); write_byte(q); delay0(1);//LCD上数值跳变的数度 &#125;//**************************dht11测试某块*************************************// void start()//开始信号 &#123; io=1; delay1(); io=0; delay0(25);// 主机把总线拉低必须大于18msDHT11能检测到起始信号 io=1; delay1();//以下三个延时函数差不多为24us delay1(); delay1(); &#125; uchar receive_byte()//接收一个字节// &#123; uchar i,temp; for(i=0;i&lt;8;i++)//接收8bit的数据 &#123; while(!io);//等待50us的低电平开始信号结束 delay1();//26us-28us delay1(); delay1(); temp=0;//时间为26us-28us数据'0' if(io==1) temp=1; //如果26us-28us'1' while(io);//'0'为26us-28us'1'为70us data_byte&lt;&lt;=1;// data_byte|=temp; &#125; return data_byte; &#125; int receive(int select)//接收数据// bb&#123; int result; uchar T_H,T_L,R_H,R_L,check,num_check,i; start();//开始信号// io=1; //DHT11 if(!io)//判断从机是否有低电平响应信号// &#123; while(!io);//判断从机发出 80us 的低电平响应信号是否结束// while(io);//判断从机发出 80us R_H=receive_byte();//湿度高位 R_L=receive_byte();//湿度低位 T_H=receive_byte();//温度高位 T_L=receive_byte();//温度低位 check=receive_byte();//校验位 io=0; //当最后一bit50us// for(i=0;i&lt;7;i++)//差不多50us的延时 delay1(); io=1;// num_check=R_H+R_L+T_H+T_L; if(num_check==check)//判断读到的四个数据之和是否与校验位相同 &#123; RH=R_H; RL=R_L; TH=T_H; TL=T_L; check=num_check; &#125; &#125; if(select==0) result=RH; if(select==1) result=TH; return result; &#125; void display_all()&#123; display(0x00,'w'); display(0x01,'e'); display(0x02,'n'); display(0x03,'d'); display(0x04,'u');//LCD的第一行显示 display(0x05,':'); display(0x06,RH/10+0x30); //0x30LCD1602中0x30的位置放有数字0RH/10+0x30即表示湿度的十位数字在字库RH/10+0x30 display(0x07,RH%10+0x30); display(0X08,'%'); display(0x40,'s'); display(0x41,'h'); display(0x42,'i'); display(0x43,'d'); display(0x44,'u');//LCD的第二行显示 display(0x45,':'); display(0x46,TH/10+0x30); display(0x47,TH%10+0x30); display(0x48,0xdf);//以下两个是温度单位的处理 display(0x49,0x43);&#125; 8266WIFI模块和连接物联网Onenet平台的代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141#include &lt;reg52.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt; #include"wifi.h"#define uchar unsigned char#define uint unsigned intuchar a;char text[5] = &#123;0&#125;;char send_buf[25]="&#123;\"dht11\":";int i;void delay(unsigned int xms) &#123; unsigned int i,j; for(i=xms;i&gt;0;i--) //i=xms即延时约xms毫秒 for(j=112;j&gt;0;j--);&#125;void SendByte(unsigned char dat)&#123; ES = 0; TI = 0; SBUF = dat; while(!TI); TI = 0; ES = 1; &#125;void SendStr(unsigned char *s)&#123; while(*s!=0xff) &#123; SendByte(*s); delay(5); s++; &#125;&#125;void SendStr1(unsigned char *e)&#123; while(*e!='\0')// \0 表示字符串结束标志， //通过检测是否字符串末尾 指令码 &#123; SendByte(*e); delay(5); e++; &#125;&#125;void SendStr2(unsigned char *w)&#123; while(*w!=0xff) &#123; SendByte(*w); delay(5); w++; &#125;&#125;void wifi_init()&#123; SCON = 0x50; // SCON: 模式 1, 8-bit UART, 使能接收 TMOD = 0x20; // TMOD: timer 1, mode 2, 8-bit 重装,timer0：mode2 , 8位重装 TH1 = 0xfd; TL1 = 0Xfd; // TH1: 重装值 9600 波特率 晶振 11.0592MHz //打开总中断 EA = 1; ES = 1; //打开串口中断 TR1 = 1; delay(5); SendStr1("AT\r\n"); delay(2000); SendStr1("AT+CWMODE=3\r\n"); delay(2000); SendStr1("AT+RST\r\n"); delay(2000); SendStr1("AT+CWJAP=\"连接wifi的名称\",\"wifi密码\"\r\n"); delay(8000); SendStr1("AT+CIPSTART=\"TCP\",\"183.230.XX.34\",80\r\n"); delay(5000); SendStr1("AT+CIPMODE=1\r\n"); delay(5000); SendStr1("AT+CIPSEND\r\n");&#125;void onenet(int result_RH,int result_TH)&#123; int i=0; while(send_buf[i++]!='\0') send_buf[i-1]=0; while(text[i++]!='\0') text[i-1] = '\0'; SendStr1("POST /devices/29471064/datapoints?type=3 HTTP/1.1\r\n"); SendStr1("api-key:EAKhhHiXXXXXXXuUOHOf0bttS0=\r\n"); SendStr1("Host:api.heclouds.com\r\n"); SendStr1("Content-Length:12\r\n"); SendStr1("\r\n"); strcat(send_buf,"&#123;\"shidu\":"); sprintf(text,"%d",result_RH); strcat(send_buf,text); SendStr1(send_buf); SendStr1("&#125;\r\n"); i=0; while(send_buf[i++]!='\0') send_buf[i-1]=0; while(text[i++]!='\0') text[i-1] = '\0'; SendStr1("POST /devices/29471064/datapoints?type=3 HTTP/1.1\r\n"); SendStr1("api-key:EAKhhHiXXXXXXXuUOHOf0bttS0=\r\n"); SendStr1("Host:api.heclouds.com\r\n"); SendStr1("Content-Length:12\r\n"); SendStr1("\r\n"); strcat(send_buf,"&#123;\"wendu\":"); sprintf(text,"%d",result_TH); strcat(send_buf,text); SendStr1(send_buf); SendStr1("&#125;\r\n");&#125; void usart() interrupt 4&#123; P1 = 0XFF; if(RI == 1) //RI 接收中断 &#123; a = SBUF; if(a == '1') &#123; P1 = 0; &#125;else if(a == '0') &#123; P1 = 0xff; &#125; &#125; RI = 0;&#125; 连接Onenet平台 主函数1234567891011121314151617181920212223242526272829303132333435363738#include&lt;reg52.h&gt;#include &lt;intrins.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include"dht11.h"#include"wifi.h"sbit beep=P2^3;//int result_RH,result_TH,s,num;void main()//主函数模块// &#123; /* TMOD=0x01; TH0=(65536-45872)/256; TL0=(65536-45872)%256;// ET0=1;// TR0=1; EA=1; */ int result_RH,result_TH,s; lcd_init();//初始化LCD wifi_init(); while(1) &#123; s=0; result_RH=receive(s);//接收数据 s=1; result_TH=receive(s);//接收数据 onenet(result_RH,result_TH); display_all(); if(result_RH&lt;10||result_RH&gt;80||result_TH&lt;10||result_TH&gt;50)&#123; while(1)&#123; beep=~beep; delay(10); &#125; &#125; delay(5000); &#125; &#125;]]></content>
      <categories>
        <category>单片机</category>
      </categories>
      <tags>
        <tag>单片机</tag>
        <tag>C</tag>
      </tags>
  </entry>
</search>
